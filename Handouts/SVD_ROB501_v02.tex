%\documentclass[11pt,twoside]{nsf_jwg} %!PN
\documentclass[letterpaper]{article}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{epsfig,float,alltt}
\usepackage{psfrag,xr}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{pdfpages}
%\includepdfset{pagecommand=\thispagestyle{fancy}}

%
%***********************************************************************
%               New Commands
%***********************************************************************
%
%
\newcommand{\rb}[1]{\raisebox{1.5ex}{#1}}
 \newcommand{\trace}{\mathrm{trace}}
\newcommand{\real}{\mathbb R}  % real numbers  {I\!\!R}
\newcommand{\nat}{\mathbb R}   % Natural numbers {I\!\!N}
\newcommand{\cp}{\mathbb C}    % complex numbers  {I\!\!\!\!C}
\newcommand{\ds}{\displaystyle}
\newcommand{\mf}[2]{\frac{\ds #1}{\ds #2}}
\newcommand{\book}[2]{{Luenberger, Page~#1, }{Prob.~#2}}
\newcommand{\spanof}[1]{\textrm{span} \{ #1 \}}
 \newcommand{\cov}{\mathrm{cov}}
 \newcommand{\E}{\mathcal{E}}
\parindent 0pt
%
%
%***********************************************************************
%
%               End of New Commands
%
%***********************************************************************
%

\begin{document}


%\baselineskip=48pt  % Enforce double space

\baselineskip=48pt

\setlength{\parskip}{.3in}
\setlength{\itemsep}{.3in}

\pagestyle{plain}

{\Large \bf
\begin{center}
Rob 501 Handout: Grizzle \\
\mbox{} \\
The SVD and Numerical Rank of a Matrix\\
(Based on a handout of Prof. Freudenberg)
\end{center}
}

%\maketitle

%\vspace*{-1in}
%\section*{\mbox{}}

\Large


\noindent \textbf{Motivation:} In abstract linear algebra, a set of vectors is either linearly independent or not. There is nothing in between. For example, the set of vectors
$$\left\{ v^1 =  \left[ \begin{array}{l} 1 \\1 \end{array} \right] ,  v^2 =  \left[ \begin{array}{l} 0.999 \\1\end{array} \right] \right\}$$
is linearly independent. In this case, you look at it and say, yes, BUT, the vectors are ``almost'' dependent because when I take the determinant
$$ \det  \left[ \begin{array}{ll} 1 & 0.999 \\1 & 1\end{array} \right] = 0.001,$$
I get something pretty small, so I am OK with calling them dependent. Well, what about the set
$$\left\{ v^1 =  \left[ \begin{array}{l} 1 \\0 \end{array} \right] ,  v^2 =  \left[ \begin{array}{l} 10^4 \\1 \end{array} \right] \right\} ?$$
When you form the matrix and check the determinant, you get
$$ \det  \left[ \begin{array}{ll} 1 & 10^4 \\0& 1\end{array} \right] = 1,$$
which seems pretty far from zero. So are these vectors ``adequately'' linearly independent?

\textbf{Maybe not!} Let's note that
 $$\left[ \begin{array}{ll} 1 & 10^4 \\0& 1\end{array} \right] +  \left[ \begin{array}{ll} 0 & 0 \\10^{-4}& 0\end{array} \right]  =  \left[ \begin{array}{ll} 1 & 10^4 \\10^{-4}& 1\end{array} \right],$$
 is clearly singular! Hence, we can add a very small perturbation to our vectors and make them dependent! This cannot be good! :(

\noindent \textbf{Question:} How to quantify the statement,``the rank is \textit{nearly} 1'' or more generally, how to quantify that a set of vectors is \textit{nearly} linearly dependent?

\noindent \textbf{Answer:} The Singular Value Decomposition (\textbf{SVD}).\\


\noindent \underline{A good reference on numerical linear algebra is} G. H. Golub and C. F. van Loan, \textit{Matrix Computations}, The Johns Hopkins University Press, 1983.


\noindent \textbf{Remark:} In practice, you may have a need to deal with matrices that have complex entries, so the end of the handout also does things for $\cp^{m \times n}$. The generalization of a real \textit{symmetric} matrix is called a \textit{Hermitian} matrix. And the generalization of a real \textit{orthogonal} matrix is called a \textit{unitary matrix}. These will not be on any ROB 501 exam.



\noindent \textbf{Def.}~ An $m\times n$ matrix $\Sigma$ is \underline{rectangular diagonal} if $\Sigma_{ij}=0$ for $i\neq j$.  The diagonal of $\Sigma$ is
    \begin{equation*}
        \operatorname{diag}\left(\Sigma\right)=\left(\Sigma_{11},\ \Sigma_{22},\ \cdots,\ \Sigma_{pp}\right)
    \end{equation*}
    where $p:=\operatorname{min}(m,\ n)$.\\

\noindent \textbf{Examples}~ Consider rectangular matrices
$$
        \Sigma_1 = \left[ \begin{array}{rrrr}
            3 & 0 & 0 & 0\\
            0 & 4 & 0 & 0\\
            0 & 0 & -1 & 0
        \end{array}\right] ~~\text{and}~~
        \Sigma_2 =  \left[ \begin{array}{rr}
            1 & 0\\
            0 & -6\\
            0 & 0
        \end{array}\right]
$$

    Then,
$$
        \operatorname{diag}\left(\Sigma_1\right) = \begin{bmatrix} 3 & 4 & -1 \end{bmatrix}~~\text{and}~~
        \operatorname{diag}\left(\Sigma_2\right) = \begin{bmatrix} 1 & -6 \end{bmatrix}
$$

\newpage

\noindent \textbf{SVD Theorem:}~ Any $m\times n$ real matrix $A$ can be factored as
    \begin{equation*}
        A=U\Sigma V^\top
    \end{equation*}
    where
    \begin{align*}
        U &= m\times m\textnormal{ orthogonal matrix}\\
        V &= n\times n\textnormal{ orthogonal matrix}\\
        \Sigma &= m\times n\textnormal{ rectangular diagonal matrix}
    \end{align*}
  and  $\operatorname{diag}(\Sigma)=[\sigma_1,\ \sigma_2,\ \cdots,\ \sigma_p]$ satisfies $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_p\geq0$ where $p=\operatorname{min}(m,\ n)$. Moreover, the columns of $U$ are eigenvectors of $A A^\top$, the columns of $V$ are eigenvectors of $A^\top A$, and the $(\sigma_i)^2$ are eigenvalues of both $AA^\top$ and $A^\top  A$.

\noindent \textbf{Remark:}~ The entries of $\operatorname{diag}(\Sigma)$ are called \underline{singular values} of $A$.  We refer to $\sigma_i$  as the $i$'th singular value, to $u_i$ as the $i$'th left singular vector, and to $v_i$ as the $i$'th  right singular vector. The proof of the theorem is on page 12.

\newpage


\begin{verbatim}

 SVD 	Singular value decomposition.
 	[U,S,V] = SVD(X) produces a diagonal matrix S, of the same
 	dimension as X and with non-negative diagonal elements in
 	decreasing order, and unitary matrices U and V so that
 	X = U*S*V'.

  	By itself, SVD(X) returns a vector containing diag(S).

A =[
    0.8038    0.1788    0.0960
    0.8576    0.6365    0.6991
    0.1107    0.6680    0.8653
    0.9522    0.6690    0.7041
    0.6551    0.7961    0.9283];

>> [U,S,V]=svd(A)

U =
   -0.2439   -0.6486    0.1591   -0.6821   -0.1711
   -0.4940   -0.1655    0.7006    0.4508    0.1859
   -0.3688    0.6649    0.2370   -0.3156   -0.5159
   -0.5236   -0.2418   -0.5645    0.3955   -0.4385
   -0.5350    0.2268   -0.3303   -0.2747    0.6912

S =
    2.5686         0         0
         0    0.8370         0
         0         0    0.0100
         0         0         0
         0         0         0



V =
   -0.5877   -0.8020    0.1065
   -0.5375    0.2886   -0.7923
   -0.6047    0.5229    0.6007

>> U*U'

ans =
    1.0000    0.0000    0.0000   -0.0000    0.0000
    0.0000    1.0000   -0.0000    0.0000   -0.0000
    0.0000   -0.0000    1.0000   -0.0000   -0.0000
   -0.0000    0.0000   -0.0000    1.0000   -0.0000
    0.0000   -0.0000   -0.0000   -0.0000    1.0000

>> A-U*S*V'

ans =
   1.0e-15 *
         0    0.0278   -0.8327
   -0.3331         0    0.1110
   -0.2220         0    0.2220
   -0.2220    0.2220   -0.2220
   -0.1110    0.2220         0

\end{verbatim}

\vspace*{.5cm}

\boxed{\textbf{\underline{Theorem:}}~\text{rank(A) = number of nonzero singular values.}}

\vspace*{.5cm}


\boxed{\parbox{1\linewidth}{\textbf{Fact:} The \underline{numerical rank} of $A$ is the number of singular values that are larger than a given threshold. Often the threshold is chosen as a percentage of the largest singular value.}}

\newpage


\textbf{Example: $5 \times 5$ matrix}

$$A=\left[ \begin{array}{rrrrr}-32.57514& -3.89996& -6.30185& -5.67305& -26.21851\\-36.21632& -11.13521& -38.80726& -16.86330& -1.42786\\-5.07732& -21.86599& -38.27045& -36.61390& -33.95078\\-36.51955& -38.28404& -19.40680& -31.67486& -37.34390\\-25.28365& -38.57919& -31.99765& -38.36343& -27.13790\end{array}
 \right] $$

 \begin{verbatim}
 [U,Sigma,V]=svd(A);
 \end{verbatim}

 $$ \Sigma=\left[\begin{array}{rrrrr}132.459& 0.00000&0.00000&0.00000&0.00000\\0.00000&37.70811& 0.00000&0.00000&0.00000\\0.00000&0.00000&33.41836& 0.00000&0.00000\\0.00000&0.00000&0.00000&19.34060& 0.00000\\0.00000& 0.00000& 0.00000& 0.00000& 0.79164\end{array}\right] $$


Because the \textbf{smallest singular value $\sigma_5 = 0.79164$  is less than $1\%$ of the largest singular value} $\sigma_1=132.459$, in many cases, one might say that the numerical rank of $A$ was 4 instead of 5.

\textbf{This notion of numerical rank can be formalized by asking the following question:} Suppose $\text{rank}(A) = r$. How far away is $A$ from a matrix of rank strictly less than $r$?

\newpage

The numerical rank of a matrix is based on the expansion
$$A=U \Sigma V^\top = \sum_{i=1}^{p} \sigma_i u_i v_i^\top = \sigma_1 u_1 v_1^\top + \sigma_2 u_2 v_2^\top + \cdots + \sigma_p u_p v_p^\top  $$
where $p=\min\{m, n\}$, and once again, the singular values are ordered such that $\sigma_1 \ge \sigma_2\ge \cdots \ge \sigma_p.$ Each term $u_iv_i^\top$ is a rank-one matrix. The following exercises will help you understand the expansion.

\textbf{Exercises:} Suppose that $A$ is $m \times n$, $B$ is $n \times m $, and that $A=U \Sigma V^\top $ is the singular value decomposition of $A$.

\begin{itemize}

\item Partition $A$ by columns, that is, $A=\left[ A_1~|~A_2~|~\cdots~|~A_n\right]$ and $B$ by rows, that is, $B^\top=\left[ B_1^\top~|~B_2^\top~|~\cdots~|~B_n^\top\right]$. Show that
$$AB = \sum_{k=1}^n A_k B_k. $$
 \textbf{Hint:} Show that $\left[ A_kB_k \right]_{ij}= a_{ik} b_{kj}$ and recall the formula for  $\left[ AB \right]_{ij}$

\item For here and the following, let $m=n$. $U \Sigma =\left[\sigma_1 u_1~|~ \sigma_2 u_2 ~|~\cdots ~|~ \sigma_m u_m \right]$

\item $A=U \Sigma V^\top = \sum_{i=1}^{m} \sigma_i u_i v_i^\top $

\item $\forall~1 \le j \le m$, $\left[  u_i v_i^\top\right] v_j = \begin{cases} u_i & j=i \\ 0 & j \neq i\end{cases}  $

\item $A^\top A =V \Sigma^2 V^\top$

\item $A^\top A=V \Sigma^2 V^\top = \sum_{i=1}^{m} (\sigma_i)^2 v_i v_i^\top $

\item The e-values of $v_i v_i^\top$ are $\lambda_1=1$ and the rest are zero. \textbf{Hint:} Show that
$$\left[  v_i v_i^\top\right] v_j = \begin{cases} v_i & j=i \\ 0 & j \neq i\end{cases}  $$

\end{itemize}





\newpage

\noindent \textbf{Recall from HW} For a symmetric real matrix $M$, $$\max_{x^\top x = 1} x^\top M x = \lambda_{\rm max}(M)$$



\noindent \textbf{Def. (Induced matrix norm)} Given $A\in \real^{m\times n}$. Then the \textit{matrix norm induced by the Euclidean vector norm} is given by:
\begin{align}||A||_2:&=\max_{x^\top x = 1} ||Ax|| \\
& = \max_{x^\top x = 1}  \sqrt{x^\top A^\top Ax} \\
& = \sqrt{\max_{x^\top x = 1} x^\top A^\top Ax} \\
&=\sqrt{\lambda_{\text{max}}(A^\top A)}\end{align}
where $\lambda_{\text{max}}(A^\top A)$ denotes the largest eigenvalue of the matrix $A^\top A$. \textbf{(Recall that we proved in lecture that all the eigenvalues of a matrix having the form $A^\top A$ are real and non-negative.)(Also, recall HW 2)} \\



\textbf{Fact:} Suppose that $\text{rank}(A) = r$, so that $\sigma_r$ is the smallest non-zero singular value. Then

\begin{enumerate}
\setlength{\itemsep}{.1in}
\renewcommand{\labelenumi}{(\roman{enumi})}
\item if an $n \times m$ matrix $E$ satisfies $||E|| < \sigma_r$, then $\text{rank} (A+E)\ge r.$

\item there exists $E$ with $||E|| = \sigma_r$ and $\text{rank}(A+E) < r$.

\item In fact, for $E=-\sigma_r u_r v_r^\top$,  $\text{rank}(A+E) =r-1$.

\item Moreover, for $E=-\sigma_r u_r v_r^\top - \sigma_{r-1} u_{r-1} v_{r-1}^\top$,  $\text{rank}(A+E) =r-2$.
\end{enumerate}

\textbf{Corollary:}  Suppose $A$ is square and invertible. Then $\sigma_r$ measures the distance from $A$ to the nearest singular matrix.

\newpage

\textbf{Example: Using A above}

\begin{verbatim}
>> d=diag(Sigma);
>> d(end)=0;
>> D=diag(d);
>> B=U*D*V';
>> E=A-B;
\end{verbatim}
$$E=\left[
\begin{array}{rrrrr}-0.04169& 0.12122& 0.09818& -0.21886& 0.05458\\0.02031& -0.05906& -0.04784& 0.10663& -0.02659\\0.01966& -0.05716& -0.04629& 0.10320& -0.02574\\0.07041& -0.20476& -0.16584& 0.36968& -0.09220\\-0.08160& 0.23728& 0.19218& -0.42839& 0.10684\end{array} \right]$$

\begin{verbatim}
>> max(sqrt(eig(E'*E)))

 0.7916

 >> [U,Sigma,V]=svd(A-E);
\end{verbatim}

$$\Sigma=\left[ \begin{array}{rrrrr}132.45977& 0.00000&0.00000&0.00000&0.00000\\0.00000&37.70811& 0.00000&0.00000&0.00000\\0.00000&0.00000&33.41836& 0.00000&0.00000\\0.00000&0.00000&0.00000&19.34060& 0.00000\\0.00000& 0.00000& 0.00000& 0.00000& 0.00000\end{array} \right] $$

\vspace*{1cm}
I added a matrix with norm 0.7916 and made the (exact)  rank drop from 4 to 5! How cool is that? It really shows that the matrix was close to a singular matrix.

\underline{Another Example:}

\input{NotSmallDet.txt}

Hence, yeah, the SVD captures the fact that $A$ is nearly singular.
\newpage

\textbf{Interesting and Useful Facts not on any ROB 501 Exam:}
\begin{enumerate}
\setlength{\itemsep}{.1in}
\renewcommand{\labelenumi}{(\alph{enumi})}
\item We have not had the time to do anything with the nullspace and range of an $m\times n$  matrix $A$; they are important subspaces.
    \vspace*{1cm}

\textbf{Nullspace:} $\mathbf{N}(A):=\{ x\in \real^n~|~ Ax = 0\}$

 \vspace*{1cm}

\textbf{Range:} $\mathbf{R}(A):=\{ y\in \real^m~|~ \exists x\in \cp^n~~\text{such that}~~ y = Ax \}$

 \vspace*{1cm}

 \item \textbf{Fact:} Let $[U,\Sigma,V]=\texttt{svd}(A);$ Then the columns of $U$ corresponding to non-zero singular values are a basis for $\mathbf{R}(A)$ and the columns of $V$ corresponding to zero singular values are a basis for $\mathbf{N}(A)$.

\item The SVD can also be used to compute an "effective" range and an "effective" nullspace of a matrix.

\item Suppose that $\sigma_1 \geq ... \geq \sigma_r > \epsilon \ge  \sigma_{r+1} \geq ... \sigma_n \geq 0$, so that $r$ is the "effective" or numerical rank of $A$. (Note the $\epsilon$ inserted between $\sigma_r$ and $\sigma_{r+1}$ to denote the break point.)

\item Let $\mathbf{R}_{\text{eff}}(A)$ and $\mathbf{N}_\text{{eff}}(A)$ denote the effective range and effective nullspace of $A$, respectively. Then we can calculate bases for these subspaces by choosing appropriate singular vectors:
$$\mathbf{R}_{\text{eff}}(A) := \spanof{u_1, ..., u_r} ~~ \text{and} ~~ \mathbf{N}_{\text{eff}}(A) := \spanof{v_{r+1}, ..., v_n}.$$
\end{enumerate}

\newpage

\begin{center}

The SVD for Real Matrices
\end{center}

\noindent \textbf{Def.}~ An $m\times n$ matrix $\Sigma$ is \underline{rectangular diagonal} if $\Sigma_{ij}=0$ for $i\neq j$.  The diagonal of $\Sigma$ is
    \begin{equation*}
        \operatorname{diag}\left(\Sigma\right)=\left(\Sigma_{11},\ \Sigma_{22},\ \cdots,\ \Sigma_{kk}\right)
    \end{equation*}
    where $k=\operatorname{min}(m,\ n)$.\\

\noindent \textbf{Examples}~ Consider rectangular matrices
$$
        \Sigma_1 = \left[ \begin{array}{rrrr}
            3 & 0 & 0 & 0\\
            0 & 4 & 0 & 0\\
            0 & 0 & -1 & 0
        \end{array}\right] ~~\text{and}~~
        \Sigma_2 =  \left[ \begin{array}{rr}
            1 & 0\\
            0 & -6\\
            0 & 0
        \end{array}\right]
$$

    Then,
$$
        \operatorname{diag}\left(\Sigma_1\right) = \begin{bmatrix} 3 & 4 & -1 \end{bmatrix}~~\text{and}~~
        \operatorname{diag}\left(\Sigma_2\right) = \begin{bmatrix} 1 & -6 \end{bmatrix}
$$

\noindent \textbf{SVD Theorem:}~ Any $m\times n$ real matrix $A$ can be factored as
    \begin{equation*}
        A=U\Sigma V^\top
    \end{equation*}
    where
    \begin{align*}
        U &= m\times m\textnormal{ orthogonal matrix}\\
        V &= n\times n\textnormal{ orthogonal matrix}\\
        \Sigma &= m\times n\textnormal{ rectangular diagonal matrix}
    \end{align*}
  and  $\operatorname{diag}(\Sigma)=[\sigma_1,\ \sigma_2,\ \cdots,\ \sigma_p]$ satisfies $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_p\geq0$ where $p=\operatorname{min}(m,\ n)$. Moreover, the columns of $U$ are eigenvectors of $A A^\top$, the columns of $V$ are eigenvectors of $A^\top A$, and the $(\sigma_i)^2$ are eigenvalues of both $AA^\top$ and $A^\top  A$.

\noindent \textbf{Remark:}~ The entries of $\operatorname{diag}(\Sigma)$ are called \underline{singular values} of $A$.

\noindent \textbf{Proof of the theorem:}~ $A^\top A$ is $n\times n$, real, and symmetric. Hence, there exist orthonormal eigenvectors $\{v^1,\ \cdots,\ v^n\}$ such that $A^\top Av^j=\lambda_jv^j$. Without loss of generality, we can assume that
    \begin{equation*}
        \lambda_1\geq\lambda_2\geq\cdots\geq\lambda_n\geq0
    \end{equation*}
    If not, we simply re-order the $v^i$'s to make it so.\\

    For $\lambda_j>0$, say $1\leq j\leq r$, we define
    \begin{align*}
        \sigma_j=\sqrt{\lambda_j}
    \end{align*}
    and
    \begin{equation*}
        q^j = \frac{1}{\sigma_j}Av^j\in\real^m
    \end{equation*}
    \underline{Claim:}~ $\left(q^i\right)^\top q^j=\delta_{ij}=\begin{cases}
            1 & i=j\\
            0 & 1\neq j
        \end{cases}$ for $1\leq i,\ j\leq r$.\\
    \underline{Proof of Claim:}
    \begin{align*}
        \left(q^i\right)^\top q^j &= \frac{1}{\sigma_i}\frac{1}{\sigma_j}\left(v^i\right)^\top A^\top Av^j\\
        &=\frac{\lambda_j}{\sigma_i\sigma_j}\left(v^i\right)^\top v^j\\
        &= \begin{cases}
                \frac{\lambda_j}{\left(\sigma_i\right)^2} & i=j\\
                0 & i\neq j
            \end{cases}\\
        &= \begin{cases}
            1 & i=j\\
            0 & 1\neq j
        \end{cases}
    \end{align*}
    \underline{End of proof of Claim.}


    If $r<m$, we can extend the $q^i$'s to an orthonormal basis for $\real^m$. Define
    \begin{align*}
        U &= [q^1\ |\ q^2\ |\ \cdots\ |\ q^m]\\
        V &= [v^1\ |\ v^2\ |\ \cdots\ |\ v^n]\\
    \end{align*}
and define $\Sigma=m\times n$ by
    \begin{equation*}
        \Sigma_{ij} = \begin{cases}
                \sigma_i\delta_{ij} & 1\leq i,\ j\leq r\\
                0 & \textnormal{otherwise}
            \end{cases}
    \end{equation*}
    Then, $\Sigma$ is rectangular diagonal with
    \begin{equation*}
        \operatorname{diag}\left(\Sigma\right)=[\sigma_1,\ \sigma_2,\ \cdots,\ \sigma_r,\ 0,\ \cdots,\ 0]
    \end{equation*}

   To complete the proof of the theorem, it is enough to show that $ U^\top AV = \Sigma$. We note that the $ij$ element of this matrix is
$$        (U^\top AV)_{ij} = q_i^\top Av^j
$$
    If $j>r$, then $Av^j=0$, and thus $q_i^\top Av^j=0$, as required. If $i>r$, then $q^i$ was selected to be orthogonal to
    \begin{equation*}
        \{q^1,\ \cdots,\ q^r\}=\{\frac{1}{\sigma_1}Av^1,\ \frac{1}{\sigma_2}Av^2,\ \cdots,\ \frac{1}{\sigma_r}Av^r\}
    \end{equation*}
    and thus $\left(q^i\right)^\top Av^j=0$.\\

   Hence we now consider $1\leq i,\ j\leq r$ and compute that
    \begin{align*}
        \left(U^\top AV\right)_{ij} &= \frac{1}{\sigma_i}\left(v^i\right)^\top A^\top Av^j\\
        &= \frac{\lambda_j}{\sigma_i}\left(v^i\right)^\top v^j\\
        &= \sigma_i\delta_{ij}
    \end{align*}
    as required. \textbf{End of Proof}.

 \newpage

 {\Large \bf
\begin{center}
SVD for Complex Matrices\\
(not on ROB 501 Final Exam)
\end{center}
}

 \noindent \textbf{Hermitian:}
 Consider $x\in \cp^n$. Then we define the vector "$x$ Hermitian" by $x^H:=\bar{x}^\top$. That is, $x^H$ is the complex conjugate transpose of $x$. Similarly, for a matrix $A\in \cp^{m\times n}$, we define $A^H \in \cp^{n\times m}$ by $\bar{A}^\top$. We say that a square matrix  $A\in \cp^{n\times n}$ is a \textit{Hermitian matrix} if $A=A^H$.

\noindent \textbf{Important things to note:}
\begin{itemize}
\item Similar to $A^\top A$ for real matrices, when $A$ is complex, $A^HA$ has e-values that are real and non-negative. The proof is similar to things we have done in lecture; if you care to see it, you can find it online.

\item In MATLAB, $A'= A^H$. Yikes! It is not the ordinary transpose? No, it is the complex conjugate transpose. If you want the ordinary transpose, use \texttt{transpose(A)}.
\end{itemize}



\input{HermitianMatrix.txt}

\vspace*{2cm}

\noindent \textbf{Inner product on $\cp^n$:} Given $x,y \in \cp^n$. Let the elements $x$ and $y$ be noted
$$x=\begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} ~~\mbox{and} ~~y=\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}.$$
Then the Euclidean inner product is defined as
\begin{align}
<x,y>:&=x^H y \\ &=\bar{x}_1 y_1 +\bar{x}_2 y_2+\cdots+\bar{x}_n y_n
\end{align}
We note that this puts the linearity on the right side of the ``bracket'', but as we have noted in HW, both definitions are common.\\



\noindent \underline{Euclidean vector norm:} As in class, the vector norm associated with this inner product is given by
\begin{align}||x||_2:&=\sqrt{<x,x>} \\
&=\sqrt{\sum\limits_{i=1}^n |x_i|^2 }\end{align}
We often omit the subscript "2"  when we are discussing the Euclidean norm (or "2-norm") exclusively.

\noindent \underline{Euclidean matrix norm:} Given $A\in \cp^{m\times n}$. Then the \textit{matrix norm induced by the Euclidean vector norm} is given by:
\begin{align}||A||_2:&=\max_{x^Hx = 1} ||Ax|| \\
& = \max_{x^Hx = 1}  \sqrt{x^HA^HAx} \\
& = \sqrt{\max_{x^Hx = 1} x^HA^HAx} \\
&=\sqrt{\lambda_{\text{max}}(A^H A)}\end{align}
where $\lambda_{\text{max}}(A^H A)$ denotes the largest eigenvalue of the matrix $A^H A$. \textbf{(As noted above, all the eigenvalues of a matrix having the form $A^H A$ are real and non-negative.)(Also, recall HW 2)}

\noindent \underline{Orthogonality:} Two vectors $x,y\in \cp^n$ are \textit{orthogonal} if $<x,y>=0$.

\noindent \underline{Orthonormal Set:} A collection of vectors $\{x_1,x_2,\cdots,x_m\}\in \cp^n$ is said to be an \textit{orthonormal set} if
$$<x_i,x_j>=\left\{\begin{array}{ll}
0, & i\not=j\\
1, & i=j
\end{array}\right.~~~~~(\text{Hence}~||x_i||=1, \forall~i.)$$

\noindent \textbf{\underline{Unitary Matrix:}} A matrix $U \in \cp^{n\times n}$ is \textit{unitary} if $U^H U = U U^H=I_n$.

\newpage

\noindent \textbf{\underline{Fact:}} If $U$ is a unitary matrix, then the columns of $U$ form an orthonormal basis (ONB) for $\cp^n$.

\noindent \underline{Proof of Fact:} Denote the columns of $U$ as $U=\begin{bmatrix}u_1 & u_2 & \cdots & u_n \end{bmatrix}$. Then
$$U^H U =\begin{bmatrix}u^H_1 \\[2ex] u^H_2 \\ \vdots \\[2ex] u^H_n \end{bmatrix}\begin{bmatrix}u_1 & u_2 & \cdots & u_n \end{bmatrix}=\left[\begin{array}{cccc} u^H_1 u_1 & u^H_1 u_2 & \cdots & u^H_1 u_n \\[2ex] u^H_2 u_1 & u^H_2 u_2 & \cdots & u^H_2 u_n \\[2ex] \vdots & \vdots &\ddots & \vdots \\[2ex] u^H_n u_1 & u^H_n u_2 & \cdots & u^H_n u_n\end{array}\right]=\left[\begin{array}{cccc} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots &\ddots & \vdots \\ 0 & 0 & \cdots & 1\end{array}\right]$$

\vspace*{1cm}

\textbf{For real matrices, unitary is the same thing as orthogonal}.

\noindent \textbf{\underline{Example:}}
\input{UnitaryMartix.txt}

\newpage

\noindent \textbf{\underline{Unitary matrices are effectively rotation matrices:}} they do not change the length of a vector, nor the angle between two vectors. Indeed,
\vspace*{-.2in}
\begin{enumerate}
\setlength{\itemsep}{1cm}
\item[1)] From $U^H U = U U^H=I_n$, it follows that $U^{-1} =U^H$
\item[2)]  Let's compute the inner product of $U x$ and $U y$:
    $$ <U x,U y>:=(U x)^H U y = x^H U^H U y = x^H y = :<x,y>$$
\item[3)] It follows that \\
    (a) norm of $U x$ equals the norm of $x$:
       $$||U x||^2: = <U x,U x>=<x,x>=:||x||^2$$
    (b) angle between $x$ and $y$ is the same as the angle between $U x$ and $U y$:
    $$\cos(\angle(x,y)):=\frac{<x,y>}{||x||~ ||y||}=\frac{<U x,U y>}{||U x||~ ||U y||} =: \cos(\angle(U x,U y))$$
\item[4)] All of the e-values of $U$ have magnitude 1. Indeed, suppose that $\lambda$ is an e-value with e-vector $v$: $Uv=\lambda v$ \\
    \\
Applying norms to both sides of the above yields: $||Uv||=||\lambda v||$ \\
\\
But, by item (3) above and properties of norms:
$$ ||Uv||=||v||~\text{and}~ ||\lambda v||=|\lambda|~||v||$$
which, with the above, implies $|\lambda|=1$.
\end{enumerate}

\textbf{Theorem (SVD for Complex matrices:} Consider $A\in \cp^{m \times n}$. Then there exist unitary matrices
$$\begin{aligned}
U&=\begin{bmatrix}u_1 & u_2 & \cdots & u_m \end{bmatrix} \\
V&=\begin{bmatrix}v_1 & v_2 & \cdots & v_n \end{bmatrix}
\end{aligned}$$
such that
$$A=\left\{\begin{array}{ll}
U\begin{bmatrix} \Sigma \\[2ex] 0 \end{bmatrix}V^H, & m\ge n \vspace{5pt}\\
U\begin{bmatrix} \Sigma & 0 \end{bmatrix}V^H, & m\le n
\end{array}\right.$$
where
$$\Sigma=\left[\begin{array}{cccc} \sigma_1 & 0 & \cdots & 0 \\ 0 & \sigma_2 & \cdots & 0 \\ \vdots & \vdots &\ddots & \vdots \\ 0 & 0 & \cdots & \sigma_{p}\end{array}\right],~p=\text{min}(m,n)$$
and
$$\sigma_1\ge\sigma_2\ge\cdots\sigma_{p}\ge0$$
\noindent \underline{Terminology:} We refer to $\sigma_i$  as the $i$'th singular value, to $u_i$ as the $i$'th left singular vector, and to $v_i$ as the $i$'th  right singular vector. \\
\\
\\


\end{document} 